# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Load the dataset from uploaded file
path = '/content/Advertising.csv'
data = pd.read_csv(path)

# Display first few rows of the dataset
data.head()
# Checking basic information of the dataset
data.info()

# Statistical summary of numerical features
data.describe()

# Pairplot to visualize the relationships between features
sns.pairplot(data)
plt.show()
# Correlation Heatmap
plt.figure(figsize=(8,6))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix of Features')
plt.show()
# Check for missing values
data.isnull().sum()

# Drop missing values if any (optional, based on data)
data = data.dropna()

# Defining Features (X) and Target (y)
X = data.drop('Sales', axis=1)  # Features: 'TV', 'Radio', 'Newspaper'
y = data['Sales']  # Target: Sales

# Splitting the data into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Predictions on test set
y_pred_lr = lr_model.predict(X_test)

# Evaluation metrics
def evaluate_model(y_true, y_pred, model_name):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    print(f"Model: {model_name}")
    print(f"RMSE: {rmse}")
    print(f"MAE: {mae}")
    print(f"R^2 Score: {r2}")
    print("-" * 50)

# Evaluate Linear Regression
evaluate_model(y_test, y_pred_lr, "Linear Regression")
# Random Forest Model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions on test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate Random Forest
evaluate_model(y_test, y_pred_rf, "Random Forest")
# Random Forest Model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions on test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate Random Forest
evaluate_model(y_test, y_pred_rf, "Random Forest")
# Plotting Actual vs Predicted for Random Forest
plt.figure(figsize=(10,6))
plt.scatter(y_test, y_pred_rf, color='blue', label='Predicted')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', lw=2, label='Perfect Fit')
plt.title('Random Forest: Actual vs Predicted Sales')
plt.xlabel('Actual Sales')
plt.ylabel('Predicted Sales')
plt.legend()
plt.show()
# Residuals: Difference between actual and predicted
residuals_rf = y_test - y_pred_rf

# Residual plot for Random Forest
plt.figure(figsize=(10,6))
sns.histplot(residuals_rf, kde=True, color='purple')
plt.title('Distribution of Residuals for Random Forest')
plt.xlabel('Residuals')
plt.show()
# Re-evaluate Linear Regression
print("Evaluation of Linear Regression Model:")
evaluate_model(y_test, y_pred_lr, "Linear Regression")

# Re-evaluate Random Forest
print("Evaluation of Random Forest Model:")
evaluate_model(y_test, y_pred_rf, "Random Forest")

# Feature Importance from Random Forest
importances = rf_model.feature_importances_
feature_names = X.columns

# Plotting Feature Importance
plt.figure(figsize=(8,6))
sns.barplot(x=importances, y=feature_names)
plt.title('Feature Importance in Random Forest')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.show()

# Print feature importance scores
for name, importance in zip(feature_names, importances):
    print(f"{name}: {importance}")

from sklearn.ensemble import GradientBoostingRegressor

# Initialize the Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)

# Train the model
gb_model.fit(X_train, y_train)

# Predictions
y_pred_gb = gb_model.predict(X_test)

# Evaluation for Gradient Boosting
evaluate_model(y_test, y_pred_gb, "Gradient Boosting Regressor")
# Installing XGBoost if not already installed (for Google Colab use !pip install xgboost)
# !pip install xgboost

import xgboost as xgb

# Initialize XGBoost Regressor
xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)

# Train the model
xgb_model.fit(X_train, y_train)

# Predictions
y_pred_xgb = xgb_model.predict(X_test)

# Evaluation for XGBoost
evaluate_model(y_test, y_pred_xgb, "XGBoost Regressor")
from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

# Initialize Random Forest with GridSearchCV
rf_grid = GridSearchCV(estimator=RandomForestRegressor(random_state=42), param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit to training data
rf_grid.fit(X_train, y_train)

# Best parameters from grid search
print("Best Parameters for Random Forest:", rf_grid.best_params_)

# Predictions using the best model
y_pred_rf_tuned = rf_grid.best_estimator_.predict(X_test)

# Evaluate the tuned Random Forest
evaluate_model(y_test, y_pred_rf_tuned, "Tuned Random Forest")
